Cluster.I = Cluster.I,
dimRed.B,
dimRed.I,
Epitope = as.vector(SeuratMeta$Epitope))
plot.df$Epitope[which(is.na(plot.df$Epitope))] <- "No"
plot.df$Epitope <- factor(plot.df$Epitope, levels = c("NP", "ORF8", "RBD", "Spike", "No"))
plot.df$Epitope[which(is.na(plot.df$Epitope))] <- "No"
ggplot(plot.df, aes(x = Ben_UMAP1, y=Ben_UMAP2)) +
geom_point(aes(color = Cluster.B)) +
scale_color_viridis(discrete = TRUE, option = "H", na.value = "grey") +
guides(color = "none") +
theme_void()
library(viridis)
ggplot(plot.df %>% arrange(desc(Epitope)), aes(x = Ben_UMAP1, y=Ben_UMAP2)) +
geom_point(aes(color = Epitope)) +
scale_color_manual(values = c(viridis_pal()(4), "grey")) +
guides(color = "none") +
theme_void()
colnames(plot.df)
plot.df <- cbind.data.frame(Cluster.B = Cluster.B,
Cluster.I = Cluster.I,
dimRed.B,
dimRed.I,
Epitope = as.vector(SeuratMeta$Epitope),
Heavy.V = SeuratMeta$Heavy.V,
Light.V = SeuratMeta$Light.V)
plot.df$Epitope[which(is.na(plot.df$Epitope))] <- "No"
plot.df$Epitope <- factor(plot.df$Epitope, levels = c("NP", "ORF8", "RBD", "Spike", "No"))
plot.df$Epitope[which(is.na(plot.df$Epitope))] <- "No"
ggplot(plot.df %>% arrange(desc(Epitope)), aes(x = Ben_UMAP1, y=Ben_UMAP2)) +
geom_point(aes(color = Heavy.V)) +
scale_color_manual(values = c(viridis_pal()(4), "grey")) +
guides(color = "none") +
theme_void()
ggplot(plot.df %>% arrange(desc(Epitope)), aes(x = Ben_UMAP1, y=Ben_UMAP2)) +
geom_point(aes(color = Heavy.V)) +
scale_color_viridis(discrete = TRUE)
ggplot(plot.df %>% arrange(desc(Epitope)), aes(x = Ben_UMAP1, y=Ben_UMAP2)) +
geom_point(aes(color = Heavy.V)) +
scale_color_viridis(discrete = TRUE) +
guides(color = "none") +
theme_void()
ggplot(plot.df %>% arrange(desc(Epitope)), aes(x = Ben_UMAP1, y=Ben_UMAP2)) +
geom_point(aes(color = Light.V)) +
scale_color_viridis(discrete = TRUE) +
guides(color = "none") +
theme_void()
ggplot(plot.df %>% arrange(desc(Epitope)), aes(x = Ben_UMAP1, y=Ben_UMAP2)) +
geom_point(aes(color = Heavy.V)) +
scale_color_viridis(discrete = TRUE) +
guides(color = "none") +
theme_void()
ggplot(plot.df %>% arrange(desc(Epitope)), aes(x = Ibex_UMAP1, y=Ibex_UMAP2)) +
geom_point(aes(color = Heavy.V)) +
scale_color_viridis(discrete = TRUE) +
guides(color = "none") +
theme_void()
ggplot(plot.df %>% arrange(desc(Epitope)), aes(x = Ibex_UMAP1, y=Ibex_UMAP2)) +
geom_point(aes(color = SeuratMeta$Heavy.C)) +
scale_color_viridis(discrete = TRUE) +
guides(color = "none") +
theme_void()
ggplot(plot.df, aes(x = Ben_UMAP1, y=Ben_UMAP2)) +
geom_point(aes(color = SeuratMeta$Heavy.C)) +
scale_color_viridis(discrete = TRUE, option = "H", na.value = "grey") +
guides(color = "none") +
theme_void()
out <- trainModels(test, factor, c("Spike"), split_ratio = 0.7)
library(caret)
library(dplyr)
library(pROC)
library(xgboost)
library(nnet)
# Function to tune and train models using caret
trainModels <- function(data_matrix,
target_vector,
class,
split_ratio = 0.7,
use_smote = TRUE) {
# Binarize the target vector (assumes binary classification: 0 and 1)
binary_target <- as.factor(ifelse(target_vector %in% class, "Yes", "No"))
# Convert data matrix to a data frame and add the binary target
df <- as.data.frame(data_matrix)
df$Target <- binary_target
# Split data into training and validation sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$Target, p = split_ratio, list = FALSE)
train_data <- df[train_index, ]
validation_data <- df[-train_index, ]
if (use_smote) {
train_data <- DMwR::SMOTE(Target ~ ., data = train_data, perc.over = 100, perc.under = 200)
}
# Define train control with 5-fold cross-validation
train_control <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# Define a list of models to train
models <- list(
"Logistic Regression" = "glm",
"Random Forest" = "rf",
"Support Vector Machine" = "svmRadial",
"K-Nearest Neighbors" = "knn",
"Naive Bayes" = "nb",
"Gradient Boosting Machine" = "gbm",
"Extreme Gradient Boosting" = "xgbTree",
"Neural Network (MLP)" = "nnet",
"Decision Tree (rpart)" = "rpart",
"Linear Discriminant Analysis" = "lda"
)
# Initialize a list to store model results
model_results <- list()
# Loop through each model type
for (model_name in names(models)) {
set.seed(123)  # For reproducibility
# Define class weights for the model if applicable
if (model_name %in% c("Logistic Regression", "Support Vector Machine")) {
weights <- ifelse(train_data$Target == 1, 10, 1)
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
weights = weights,  # Apply class weights
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
} else {
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
}
# Predict on the validation set
validation_predictions <- predict(model, validation_data)
# Store results including validation predictions
model_results[[model_name]] <- list(
model = model,
validation_predictions = validation_predictions,
validation_true = validation_data$Target,
confusion_matrix = confusionMatrix(validation_predictions, validation_data$Target, positive = "Yes")
)
}
return(model_results)
}
out <- trainModels(test, factor, c("Spike"), split_ratio = 0.7)
test <- Ibex.IGH[1:2000,]
factor <- SeuratMeta$Epitope[1:2000]
out <- trainModels(test, factor, c("Spike"), split_ratio = 0.7)
# Binarize the target vector (assumes binary classification: 0 and 1)
binary_target <- as.factor(ifelse(target_vector %in% class, "Yes", "No"))
target_vector <- factor
data_matrix <- test
class <- c("Spike")
split_ratio = 0.7,
use_smote = TRUE
# Binarize the target vector (assumes binary classification: 0 and 1)
binary_target <- as.factor(ifelse(target_vector %in% class, "Yes", "No"))
# Convert data matrix to a data frame and add the binary target
df <- as.data.frame(data_matrix)
df$Target <- binary_target
# Split data into training and validation sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$Target, p = split_ratio, list = FALSE)
train_data <- df[train_index, ]
split_ratio = 0.7
# Split data into training and validation sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$Target, p = split_ratio, list = FALSE)
train_data <- df[train_index, ]
validation_data <- df[-train_index, ]
if (use_smote) {
train_data <- DMwR::SMOTE(Target ~ ., data = train_data, perc.over = 100, perc.under = 200)
}
# Define train control with 5-fold cross-validation
train_control <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# Define a list of models to train
models <- list(
"Logistic Regression" = "glm",
"Random Forest" = "rf",
"Support Vector Machine" = "svmRadial",
"K-Nearest Neighbors" = "knn",
"Naive Bayes" = "nb",
"Gradient Boosting Machine" = "gbm",
"Extreme Gradient Boosting" = "xgbTree",
"Neural Network (MLP)" = "nnet",
"Decision Tree (rpart)" = "rpart",
"Linear Discriminant Analysis" = "lda"
)
# Initialize a list to store model results
model_results <- list()
# Loop through each model type
for (model_name in names(models)) {
set.seed(123)  # For reproducibility
# Define class weights for the model if applicable
if (model_name %in% c("Logistic Regression", "Support Vector Machine")) {
weights <- ifelse(train_data$Target == 1, 10, 1)
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
weights = weights,  # Apply class weights
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
} else {
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
}
# Predict on the validation set
validation_predictions <- predict(model, validation_data)
# Store results including validation predictions
model_results[[model_name]] <- list(
model = model,
validation_predictions = validation_predictions,
validation_true = validation_data$Target,
confusion_matrix = confusionMatrix(validation_predictions, validation_data$Target, positive = "Yes")
)
}
set.seed(123)  # For reproducibility
weights <- ifelse(train_data$Target == 1, 10, 1)
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
weights = weights,  # Apply class weights
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
train_data
df$Target <- binary_target
# Split data into training and validation sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$Target, p = split_ratio, list = FALSE)
train_data <- df[train_index, ]
head(train_data)
if (use_smote) {
train_data <- DMwR::SMOTE(Target ~ ., data = train_data, perc.over = 100, perc.under = 200)
}
head(train_data)
View(train_data)
train_data <- df[train_index, ]
train_data <- DMwR::SMOTE(Target ~ ., data = train_data)
View(train_data)
train_data <- df[train_index, ]
train_data <- na.omit(DMwR::SMOTE(Target ~ ., data = train_data))
View(train_data)
# Define train control with 5-fold cross-validation
train_control <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# Define a list of models to train
models <- list(
"Logistic Regression" = "glm",
"Random Forest" = "rf",
"Support Vector Machine" = "svmRadial",
"K-Nearest Neighbors" = "knn",
"Naive Bayes" = "nb",
"Gradient Boosting Machine" = "gbm",
"Extreme Gradient Boosting" = "xgbTree",
"Neural Network (MLP)" = "nnet",
"Decision Tree (rpart)" = "rpart",
"Linear Discriminant Analysis" = "lda"
)
# Initialize a list to store model results
model_results <- list()
# Loop through each model type
for (model_name in names(models)) {
set.seed(123)  # For reproducibility
# Define class weights for the model if applicable
if (model_name %in% c("Logistic Regression", "Support Vector Machine")) {
weights <- ifelse(train_data$Target == 1, 10, 1)
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
weights = weights,  # Apply class weights
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
} else {
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
}
# Predict on the validation set
validation_predictions <- predict(model, validation_data)
# Store results including validation predictions
model_results[[model_name]] <- list(
model = model,
validation_predictions = validation_predictions,
validation_true = validation_data$Target,
confusion_matrix = confusionMatrix(validation_predictions, validation_data$Target, positive = "Yes")
)
}
View(model_results)
x <- model_results[[1]]
lapply(model_results, function(x) { x[[4]]$table})
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
warnings
warnings()
train_data <- df[train_index, ]
test <- na.omit(DMwR::SMOTE(Target ~ ., data = train_data,
perc.over = 300, k = 5, perc.under = 300))
table(test$Target)
test <- na.omit(DMwR::SMOTE(Target ~ ., data = train_data,
perc.over = 300, k = 5, perc.under = 500))
table(test$Target)
test <- na.omit(DMwR::SMOTE(Target ~ ., data = train_data,
perc.over = 500, k = 5, perc.under = 100))
table(test$Target)
View(test)
test <- na.omit(DMwR::SMOTE(Target ~ ., data = train_data,
perc.over = 1000, k = 5, perc.under = 100))
table(test$Target)
train_data <- na.omit(DMwR::SMOTE(Target ~ ., data = train_data,
perc.over = 1000, k = 5, perc.under = 100))
# Define train control with 5-fold cross-validation
train_control <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# Define a list of models to train
models <- list(
"Logistic Regression" = "glm",
"Random Forest" = "rf",
"Support Vector Machine" = "svmRadial",
"K-Nearest Neighbors" = "knn",
"Naive Bayes" = "nb",
"Gradient Boosting Machine" = "gbm",
"Extreme Gradient Boosting" = "xgbTree",
"Neural Network (MLP)" = "nnet",
"Decision Tree (rpart)" = "rpart",
"Linear Discriminant Analysis" = "lda"
)
# Initialize a list to store model results
model_results <- list()
# Loop through each model type
for (model_name in names(models)) {
set.seed(123)  # For reproducibility
# Define class weights for the model if applicable
if (model_name %in% c("Logistic Regression", "Support Vector Machine")) {
weights <- ifelse(train_data$Target == 1, 10, 1)
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
weights = weights,  # Apply class weights
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
} else {
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
}
# Predict on the validation set
validation_predictions <- predict(model, validation_data)
# Store results including validation predictions
model_results[[model_name]] <- list(
model = model,
validation_predictions = validation_predictions,
validation_true = validation_data$Target,
confusion_matrix = confusionMatrix(validation_predictions, validation_data$Target, positive = "Yes")
)
}
lapply(model_results, function(x) { x[[4]]$table})
library(caret)
library(dplyr)
library(pROC)
library(xgboost)
library(nnet)
# Function to tune and train models using caret
trainModels <- function(data_matrix,
target_vector,
class,
split_ratio = 0.7,
use_smote = TRUE) {
# Binarize the target vector (assumes binary classification: 0 and 1)
binary_target <- as.factor(ifelse(target_vector %in% class, "Yes", "No"))
# Convert data matrix to a data frame and add the binary target
df <- as.data.frame(data_matrix)
df$Target <- binary_target
# Split data into training and validation sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$Target, p = split_ratio, list = FALSE)
train_data <- df[train_index, ]
validation_data <- df[-train_index, ]
if (use_smote) {
train_data <- na.omit(DMwR::SMOTE(Target ~ ., data = train_data,
perc.over = 1000, k = 5, perc.under = 100))
}
# Define train control with 5-fold cross-validation
train_control <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
# Define a list of models to train
models <- list(
"Logistic Regression" = "glm",
"Random Forest" = "rf",
"Support Vector Machine" = "svmRadial",
"K-Nearest Neighbors" = "knn",
"Naive Bayes" = "nb",
"Gradient Boosting Machine" = "gbm",
"Extreme Gradient Boosting" = "xgbTree",
"Neural Network (MLP)" = "nnet",
"Decision Tree (rpart)" = "rpart",
"Linear Discriminant Analysis" = "lda"
)
# Initialize a list to store model results
model_results <- list()
# Loop through each model type
for (model_name in names(models)) {
set.seed(123)  # For reproducibility
# Define class weights for the model if applicable
if (model_name %in% c("Logistic Regression", "Support Vector Machine")) {
weights <- ifelse(train_data$Target == 1, 10, 1)
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
weights = weights,  # Apply class weights
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
} else {
model <- train(
Target ~ .,
data = train_data,
method = models[[model_name]],
trControl = train_control,
metric = "ROC",  # Optimizing for ROC AUC
preProcess = c("center", "scale"),  # Preprocessing steps
tuneLength = 10  # Adjusts the level of hyperparameter tuning
)
}
# Predict on the validation set
validation_predictions <- predict(model, validation_data)
# Store results including validation predictions
model_results[[model_name]] <- list(
model = model,
validation_predictions = validation_predictions,
validation_true = validation_data$Target,
confusion_matrix = confusionMatrix(validation_predictions, validation_data$Target, positive = "Yes")
)
}
return(model_results)
}
out <- trainModels(Ibex.IGH, SeuratMeta$Epitope, c("RBD", "Spike"), split_ratio = 0.7)
